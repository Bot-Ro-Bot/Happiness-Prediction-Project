---
title: "Happiness Score Prediction"
author: "RABIN NEPAL (U00901360)"
date: "2023-11-24"
output: pdf_document
---

# Necessary Library Imports
```{r setup, include=FALSE}
library(car)
library(psych)
library(leaps)

# KNN
library(caret) 
library(kknn)

# Ridge and Lasso Regression
library(glmnet)
library(dplyr)
library(tidyr)

# for visualization
library(corrplot)
library(ggplot2)

# define some constants
DATA_DIR = "/Users/rabinnepal/gitHub/Happiness-Prediction-Project/data/"

# YEARS = list("2015","2016","2017","2018","2019","2020","2021","2022","2023")
YEARS = 2015:2023
YEARS = paste(YEARS) # convert num array to string array
TOTAL_YEARS = length(YEARS)
VARIABLE_NAMES = c()
RESPONSE = "Happiness_score"

# set working directory 
# setwd(DATA_DIR)
knitr::opts_knit$set(root.dir = DATA_DIR)
```


# Data Import and Cleanup
```{r}
# get filenames from data directory
files = list.files(DATA_DIR)
print(files)

# read csv files for world happiness data for each year 
# data_2015 = read.csv("2015.csv")
# data_2016 = read.csv("2016.csv")
# data_2017 = read.csv("2017.csv")
# data_2018 = read.csv("2018.csv")
# data_2019 = read.csv("2019.csv")
# data_2020 = read.csv("2020.csv")
# data_2021 = read.csv("2021.csv")
# data_2022 = read.csv("2022.csv")
# data_2023 = read.csv("2023.csv")

# read csv files for world happiness data for each year 
for(filename in files){
  year = substring(filename,1,4)
  # set name of variables dynamically as data_{year}
  variable_name = paste0("data_",year)
  VARIABLE_NAMES = c(VARIABLE_NAMES,variable_name)
  # print(variable_name)
  
  # read csv file
  year_data = read.csv(filename,stringsAsFactors = F)
   # year_data = read.csv(filename,stringsAsFactors = T)
  assign(variable_name,year_data)
  
  # see dimension of data for each year
  print(paste("Shape of data for ", year, "is :", dim(year_data)[1], "rows and", dim(year_data)[2], "columns."))
}


# see samples of each year data
# head(data_2015)
for(i in 1:TOTAL_YEARS){
  print(paste("A sample of data for year",2014+i,":"))
  dummy_year = get(VARIABLE_NAMES[i])
  glimpse(dummy_year)
}


# remove null values from dataset
for(i in 1:TOTAL_YEARS){
  dummy_year = get(VARIABLE_NAMES[i])
  if(any(is.na(dummy_year))){
    print("Found NA values and removing them...")
    dummy_year = na.omit(dummy_year)
    assign(VARIABLE_NAMES[i],dummy_year)
  }
}

```



# Data Exploration (All Years)
```{r}
corrplot(corr = cor(data_2023[,-c(1,2)]),method = "color", outline = T,addCoef.col = "white", number.digits = 2, number.cex = 0.75)

# lets first see the columns for each year data
col_2015 = c(colnames(data_2015))
col_2016 = colnames(data_2016)
col_2017 = colnames(data_2017)
col_2018 = colnames(data_2018)
col_2019 = colnames(data_2019)
col_2020 = colnames(data_2020)
col_2021 = colnames(data_2021)
col_2022 = colnames(data_2022)
col_2023 = colnames(data_2023)

# lets find which columns are consistent among all these years
all_col = list(col_2015,col_2016,col_2017,col_2018,col_2019,col_2020,col_2021,col_2022,col_2023)
common_values = Reduce(intersect, all_col)
print(common_values) # gives 0 : means the columns have been named differently in the dataset

# After exploring the column names we can clearly see that the same variables are named differenty in the dataset for different years. (Ex: Country , Country.or.region , Country.name )

# lets make the column names uniform
column_names = c("Country", "Happiness.Score", "GDP.per.Capita", "Social.Support", "Life.Expectancy","Freedom", "Generosity", "Corruption")

# print(common_values) 
# gives all the values in column_names

# filter the values that are essential for the study (as mentioned on the world happiness report appendix)

for(i in 1:TOTAL_YEARS){
  dummy_year = get(VARIABLE_NAMES[i])
  # select according to columns names
  dummy_year = dummy_year[column_names]
  # sort by column name
  dummy_year = dummy_year[,order(colnames(dummy_year))]
  assign(VARIABLE_NAMES[i], dummy_year ) 
  }


# lets see which countries has held the No.1 rank in happiness over the years
happy_countries = c()
happy_countries_score = c()

sad_countries = c()
sad_countries_score = c()

for(i in 1:TOTAL_YEARS){
  dummy_year = get(VARIABLE_NAMES[i])
  happy_countries[i] = dummy_year$Country[1]
  happy_countries_score[i] = dummy_year$Happiness.Score[1]
  sad_countries[i] = dummy_year$Country[nrow(dummy_year)]
  sad_countries_score[i] = dummy_year$Happiness.Score[nrow(dummy_year)]
}

print(happy_countries)
print(sad_countries)

# see trend of happiness along the years 
barplot(happy_countries_score,names.arg=happy_countries,main = "Top Happiness Scores from 2015 to 2023", ylab = "Happiness Score",las = 2, col = 'coral')

# see trend of sadness along the years
barplot(sad_countries_score,names.arg=sad_countries,main = "Least Happiness Scores from 2015 to 2023", ylab = "Happiness Score",las = 2, col= "slateblue")


# a funcition to make the access of data easy
get_all_year_values = function(col_number){
  top_countries = c()
  top_countries_values = c()
  sad_countries = c()
  sad_countries_values = c()

  for(i in 1:TOTAL_YEARS){
    dummy_year = get(VARIABLE_NAMES[i])
    dummy_year = dummy_year[order(dummy_year[,col_number],decreasing = TRUE),]
    top_countries[i] = dummy_year$Country[1]
    top_countries_values[i] = dummy_year[1,col_number]
    sad_countries[i] = dummy_year$Country[nrow(dummy_year)]
    sad_countries_values[i] = dummy_year[nrow(dummy_year),col_number]
  }
  
  return(list(top_countries = top_countries,
              top_countries_values = as.numeric(top_countries_values),
              sad_countries = sad_countries,
              sad_countries_values = as.numeric(sad_countries_values)))
}

# see trend of corruption over the years
corruption_metric = get_all_year_values(1)
barplot(corruption_metric$top_countries_values,names.arg=corruption_metric$top_countries,main = "Highest Corruption Scores from 2015 to 2023", ylab = "Corruption Score",las = 2, col = 'coral')
barplot(corruption_metric$sad_countries_values,names.arg=corruption_metric$sad_countries,main = "Least Corruption Scores from 2015 to 2023", ylab = "Corruption Score",las = 2, col= "slateblue")


# see trend of freedom over the years
freedom_metric = get_all_year_values(3)
barplot(freedom_metric$top_countries_values,names.arg=freedom_metric$top_countries,main = "Highest Freedom Scores from 2015 to 2023", ylab = "Freedom Score",las = 2, col = 'coral')
barplot(freedom_metric$sad_countries_values,names.arg=freedom_metric$sad_countries,main = "Least Freedom Scores from 2015 to 2023", ylab = "Freedom Score",las = 2, col= "slateblue")


# see trend of gdp over the years
gdp_metric = get_all_year_values(4)
barplot(gdp_metric$top_countries_values,names.arg=gdp_metric$top_countries,main = "Highest GDP from 2015 to 2023", ylab = "GDP",las = 2, col = 'coral')
barplot(gdp_metric$sad_countries_values,names.arg=gdp_metric$sad_countries,main = "Least GDP from 2015 to 2023", ylab = "GDP",las = 2, col= "slateblue")


# see trend of genorosity over the years
generosity_metric = get_all_year_values(5)
barplot(generosity_metric$top_countries_values,names.arg=generosity_metric$top_countries,main = "Most Generous Countries from 2015 to 2023", ylab = "Freedom Score",las = 2, col = 'coral')
barplot(generosity_metric$sad_countries_values,names.arg=generosity_metric$sad_countries,main = "Least Generous Countries from 2015 to 2023", ylab = "Freedom Score",las = 2, col= "slateblue")


# see trend of life expectancy over the years
life_metric = get_all_year_values(6)
barplot(life_metric$top_countries_values,names.arg=life_metric$top_countries,main = "Country with Highest Life Expectancy from 2015 to 2023", ylab = "Life Expectancy",las = 2, col = 'coral')
barplot(life_metric$sad_countries_values,names.arg=life_metric$sad_countries,main = "Country with Least Life Expectancy from 2015 to 2023", ylab = "Life Expectancy",las = 2, col= "slateblue")


# see trend of Social support over the years
social_metric = get_all_year_values(7)
barplot(social_metric$top_countries_values,names.arg=social_metric$top_countries,main = "Country with Highest Social Support from 2015 to 2023", ylab = "Freedom Score",las = 2, col = 'coral')
barplot(social_metric$sad_countries_values,names.arg=social_metric$sad_countries,main = "Country with Lowest Social Support from 2015 to 2023", ylab = "Freedom Score",las = 2, col= "slateblue")
```


## Data Exploration (All Years) - Better Visualization
```{r}
# Function to plot line graphs with year labels and country names
plot_line_graph_pretty <- function(metric_values, countries_names, main_title, y_label, line_color) {
  years <- 2015:2023
  
  plot(metric_values, type = "n", xaxt = "n", xlab = "Years", ylab = y_label, main = main_title, col = line_color)
  axis(1, at = 1:length(years), labels = years, las = 1)
  
  for (i in 1:length(countries_names)) {
    points(x = i, y = metric_values[i], pch = 19, col = line_color)
    text(x = i, y = metric_values[i], labels = countries_names[i], pos = 3, cex = 0.7, xpd = TRUE, srt = 45, adj = c(1,1))
  }
  
  # legend("topright", legend = unique(countries_names), col = line_color, pch = 19)
}

# see trend of happiness along the years 
happiness_metric = get_all_year_values(6)
plot_line_graph_pretty(happiness_metric$top_countries_values, happiness_metric$top_countries, "Happiest Countries from 2015 to 2023", "Happiness Score", "coral")
plot_line_graph_pretty(happiness_metric$sad_countries_values, happiness_metric$sad_countries, "Unhappiest Countries from 2015 to 2023", "Happiness Score", "slateblue")

# See trend of corruption over the years
corruption_metric = get_all_year_values(1)
plot_line_graph_pretty(corruption_metric$top_countries_values, corruption_metric$top_countries, "Highest Corruption Scores from 2015 to 2023", "Corruption Score", "coral")
plot_line_graph_pretty(corruption_metric$sad_countries_values, corruption_metric$sad_countries, "Least Corruption Scores from 2015 to 2023", "Corruption Score", "slateblue")

# See trend of freedom over the years
freedom_metric = get_all_year_values(3)
plot_line_graph_pretty(freedom_metric$top_countries_values, freedom_metric$top_countries, "Highest Freedom Scores from 2015 to 2023", "Freedom Score", "coral")
plot_line_graph_pretty(freedom_metric$sad_countries_values, freedom_metric$sad_countries, "Least Freedom Scores from 2015 to 2023", "Freedom Score", "slateblue")

# See trend of GDP over the years
gdp_metric = get_all_year_values(4)
plot_line_graph_pretty(gdp_metric$top_countries_values, gdp_metric$top_countries, "Highest GDP from 2015 to 2023", "GDP", "coral")
plot_line_graph_pretty(gdp_metric$sad_countries_values, gdp_metric$sad_countries, "Least GDP from 2015 to 2023", "GDP", "slateblue")

# See trend of generosity over the years
generosity_metric = get_all_year_values(5)
plot_line_graph_pretty(generosity_metric$top_countries_values, generosity_metric$top_countries, "Most Generous Countries from 2015 to 2023", "Generosity Score", "coral")
plot_line_graph_pretty(generosity_metric$sad_countries_values, generosity_metric$sad_countries, "Least Generous Countries from 2015 to 2023", "Generosity Score", "slateblue")

# See trend of life expectancy over the years
life_metric = get_all_year_values(7)
plot_line_graph_pretty(life_metric$top_countries_values, life_metric$top_countries, "Country with Highest Life Expectancy from 2015 to 2023", "Life Expectancy", "coral")
plot_line_graph_pretty(life_metric$sad_countries_values, life_metric$sad_countries, "Country with Least Life Expectancy from 2015 to 2023", "Life Expectancy", "slateblue")

# See trend of social support over the years
social_metric = get_all_year_values(8)
plot_line_graph_pretty(social_metric$top_countries_values, social_metric$top_countries, "Country with Highest Social Support from 2015 to 2023", "Social Support Score", "coral")
plot_line_graph_pretty(social_metric$sad_countries_values, social_metric$sad_countries, "Country with Lowest Social Support from 2015 to 2023", "Social Support Score", "slateblue")

```


# Data Exploration (2023)
```{r}
# lets explore in details for a particular year
# summary of data
summary(data_2023)
#see relationship between all variables in data
pairs(data_2023[,-2],lower.panel=panel.smooth)

# see relation between happiness and different factors
cor(data_2023[,-2])

corrplot(corr = cor(data_2023[,-2]),method = "color", outline = T,addCoef.col = "white", number.digits = 2, number.cex = 0.75)

# top 5 happiest and unhappiest countries in 2023
# top_5_happy = data_2023[ order(-data_2023$Happiness.Score),]
# # top_5_happy = top_5_happy$Happiness.Score[1:5]
# 
# barplot(top_5_happy$Happiness.Score[1:5],names.arg=top_5_happy$Country[1:5], horiz=TRUE, main = "Top 5 happiest Countries in 2023", xlab = "Happiness Scores",las = 2, col = 'firebrick2')
# text(top_5_happy$Happiness.Score[1:5], 1:5, labels = top_5_happy$Happiness.Score[1:5], pos = 4, cex = 0.8, col = "black")

plot_top_bottom_5 <- function(top_5, bottom_5, legend_a, legend_b) {
  if(legend_a == "Corrupted"){
    top_5$Score = top_5[,1]
    bottom_5$Score = bottom_5[,1]
    # print("Inside if")
  }
  else{
    top_5$Score = top_5[,2]
    bottom_5$Score = bottom_5[,2]
    # print("inside else")
  }

  # Combine data
  combined_data <- rbind(top_5, bottom_5)
  bar_colors <- c(rep("tomato", 5), rep("steelblue1", 5))
  par(mar = c(5, 10, 4, 2))
  barplot(combined_data$Score,
          names.arg = combined_data$Country,
          horiz = TRUE,
          main = paste("Top 5",legend_a,"and",legend_b,"Countries"),
          xlab = "Scores",
          col = bar_colors,
          border = 'black',
          xlim = c(0, max(combined_data$Score) * 1.2),
          las = 1
  )

  legend("topright", legend = c(legend_a, legend_b), fill = c("tomato", "steelblue1"), bty = "n")
}

# top 5 happiest and unhappiest countries in 2023
top_5_happy = data_2023[order(-data_2023$Happiness.Score),][1:5,c(2,6)]
top_5_unhappy = data_2023[order(data_2023$Happiness.Score),][1:5,c(2,6)]
plot_top_bottom_5(top_5_happy, top_5_unhappy, "Happiest", "Unhappiest")

# top 5 generous and ungenerous countries in 2023
top_5_generous = data_2023[ order(-data_2023$Generosity),][1:5,c(2,5)]
top_5_ungenerous = data_2023[order(data_2023$Generosity),][1:5,c(2,5)]
plot_top_bottom_5(top_5_generous, top_5_ungenerous, "Generous", "Not Generous")


# top 5 healthiest and unhealthiest countries in 2023
top_5_healthiest = data_2023[ order(-data_2023$Life.Expectancy),][1:5,c(2,7)]
top_5_unhealthiest = data_2023[ order(data_2023$Life.Expectancy),][1:5,c(2,7)]
plot_top_bottom_5(top_5_healthiest, top_5_unhealthiest, "Healthiest", "Unhealthiest")

# top 5 richest and poorest countries in 2023
top_5_richest = data_2023[ order(-data_2023$GDP.per.Capita),][1:5,c(2,4)]
top_5_poorest = data_2023[ order(data_2023$GDP.per.Capita),][1:5,c(2,4)]
plot_top_bottom_5(top_5_richest, top_5_poorest, "Richest", "Poorest")

# top 5 free and restricted countries in 2023
top_5_free = data_2023[ order(-data_2023$Freedom),][1:5,c(2,3)]
top_5_restricted = data_2023[ order(data_2023$Freedom),][1:5,c(2,3)]
plot_top_bottom_5(top_5_free, top_5_restricted, "Free", "Restricted")

# top 5 corrupted and uncorrupted countries in 2023
top_5_corrupted = data_2023[ order(-data_2023$Corruption),][1:5,c(1,2)]
top_5_uncorrupted = data_2023[ order(data_2023$Corruption),][1:5,c(1,2)]
plot_top_bottom_5(top_5_corrupted, top_5_uncorrupted, "Corrupted", "Uncorrupted")
```


# Data Clean-up for Regression
```{r}
dataset = read.csv("2023.csv",stringsAsFactors = T)
print(paste("Shape of data is :", dim(dataset)[1], "rows and", dim(dataset)[2], "columns."))

dataset = na.omit(dataset)
print(paste("Shape of data is :", dim(dataset)[1], "rows and", dim(dataset)[2], "columns."))

# see more details of dataset
summary(dataset)
head(dataset)
pairs(dataset,lower.panel=panel.smooth)

print(colnames(dataset))

# remove response variable from dataset
trail_data = dataset[,-c(1,2,3,12)]
print(paste("Shape of data is :", dim(trail_data)[1], "rows and", dim(trail_data)[2], "columns."))

pairs(trail_data,lower.panel=panel.smooth)

corrplot(corr = cor(trail_data),method = "color", outline = T,addCoef.col = "white", number.digits = 2, number.cex = 0.75)


# it appears the columns GDP, Family Support, Freedom, Life Expectancy and Corruption are are highly correlated to our response variable (happiness score)
# lets look into more detials for these columns

high_cor_col = c("Happiness.Score", "GDP.per.Capita", "Social.Support", "Life.Expectancy","Freedom", "Generosity", "Corruption")
high_corr_data = dataset[high_cor_col]
# cor(high_corr_data,lower.panel=panel.smooth)
# pairs(high_corr_data)
corrplot(corr = cor(high_corr_data),method = "color", outline = T,addCoef.col = "white", number.digits = 2, number.cex = 0.75)



par(mfrow = c(1, 2))
plot(high_corr_data$GDP.per.Capita,high_corr_data$Happiness.Score,xlab ="GDP",ylab = "Happiness Score")
plot(high_corr_data$Social.Support,high_corr_data$Happiness.Score,xlab ="Social Support",ylab = "Happiness Score")

par(mfrow = c(1, 2))
plot(high_corr_data$Life.Expectancy,high_corr_data$Happiness.Score,xlab ="Life Expectancy",ylab = "Happiness Score")
plot(high_corr_data$Freedom,high_corr_data$Happiness.Score,xlab ="Freedom",ylab = "Happiness Score")

par(mfrow = c(1, 2))
plot(high_corr_data$Generosity,high_corr_data$Happiness.Score,xlab ="Generosity",ylab = "Happiness Score")
plot(high_corr_data$Corruption,high_corr_data$Happiness.Score,xlab ="Corruption",ylab = "Happiness Score")

# mtext("Correlation Plot",side=3,line=-3,outer=TRUE)




# append data from all the years 
# read csv files for world happiness data for each year 
for(filename in files){
  year = substring(filename,1,4)
  # set name of variables dynamically as data_{year}
  variable_name = paste0("data_",year)
  VARIABLE_NAMES = c(VARIABLE_NAMES,variable_name)
  # print(variable_name)
  
  # read csv file
   year_data = read.csv(filename,stringsAsFactors = F)
   year_data = year_data[high_cor_col]
   
   # somehow the corruption data is still being read as string
   # lets convert it manually to a number
   if(typeof(year_data$Corruption)=="character"){
     print("Corruption is character")
     year_data = year_data[ order(year_data$Corruption),][-1,]
     year_data = year_data[order(-year_data$Happiness.Score)]
   }
   year_data = na.omit(year_data)
   assign(variable_name,year_data)
  
  # see dimension of data for each year
  print(paste("Shape of data for ", year, "is :", dim(year_data)[1], "rows and", dim(year_data)[2], "columns."))
}

whole_dataset = rbind(data_2015,
                      data_2016,
                      data_2017,
                      data_2018,
                      data_2019,
                      data_2020,
                      data_2021,
                      data_2022,
                      data_2023)

print(paste("Shape of data is :", dim(whole_dataset)[1], "rows and", dim(whole_dataset)[2], "columns."))

attach(whole_dataset)
predictors = whole_dataset[,c(-1)]
response = target = whole_dataset$Happiness.Score

```
# train-test split
```{r}
set.seed(7)
n = nrow(whole_dataset)
train_index=sample(n,n*0.9)

train_features=predictors[train_index,]
train_response=response[train_index]
print(paste("Shape of Training data is :", dim(train_features)[1], "rows and", dim(train_features)[2], "columns."))

test_features=predictors[-train_index,]
test_response=response[-train_index]

print(paste("Shape of Testing data is :", dim(test_features)[1], "rows and", dim(test_features)[2], "columns."))


```


# Linear Regression Model
```{r}
lin_reg = lm(train_response~., data=train_features)
summary(lin_reg)

# see if there is multicolinearity
vif_values = vif(lin_reg)
barplot(vif_values, main = "VIF Values", horiz = TRUE)

# Sort VIF values in descending order for better visualization
sorted_vif <- sort(vif_values, decreasing = TRUE)

# Create a bar plot with improved visualization
barplot(sorted_vif, 
        horiz = TRUE, 
        main = "VIF Values for Multicollinearity Check",
        xlab = "VIF",
        col = "skyblue",  # Change bar color
        border = "black",  # Add black borders to bars
        xlim = c(0, max(sorted_vif) * 1.2),  # Extend x-axis limit for better view
        las = 1,  # Rotate y-axis labels horizontally
        names.arg = names(sorted_vif),  # Use variable names as labels
        cex.names = 0.7  # Reduce label size for better readability
)
```

# Linear Regression with Scaled Data
```{r}
# scale the data
# scaled_whole_dataset = as.vector(scale(whole_dataset))
whole_dataset.scaled = as.data.frame(scale(whole_dataset))

# attach(scaled_whole_dataset)
predictors = whole_dataset.scaled[,c(-1)]
response = target = whole_dataset.scaled$Happiness.Score

set.seed(7)
n = nrow(whole_dataset)
train_index=sample(n,n*0.9)

train_features=predictors[train_index,]
train_response=response[train_index]
print(paste("Shape of Training data is :", dim(train_features)[1], "rows and", dim(train_features)[2], "columns."))

test_features=predictors[-train_index,]
test_response=response[-train_index]

print(paste("Shape of Testing data is :", dim(test_features)[1], "rows and", dim(test_features)[2], "columns."))


lin_reg = lm(train_response~., data=train_features)
summary(lin_reg)

# see if there is multicolinearity
vif_values = vif(lin_reg)
vif_values
barplot(vif_values, main = "VIF Values", horiz = TRUE)

# Sort VIF values in descending order for better visualization
sorted_vif <- sort(vif_values, decreasing = TRUE)

# Create a bar plot with improved visualization
barplot(sorted_vif, 
        horiz = TRUE, 
        main = "VIF Values for Multicollinearity Check",
        xlab = "VIF",
        col = "skyblue",  # Change bar color
        border = "black",  # Add black borders to bars
        xlim = c(0, max(sorted_vif) * 1.2),  # Extend x-axis limit for better view
        las = 1,  # Rotate y-axis labels horizontally
        names.arg = names(sorted_vif),  # Use variable names as labels
        cex.names = 0.7  # Reduce label size for better readability
)





```



# Linear Regression without GDP
```{r}
# scale the data
# scaled_whole_dataset = as.vector(scale(whole_dataset))
whole_dataset.scaled = as.data.frame(scale(whole_dataset))

# attach(scaled_whole_dataset)
predictors = whole_dataset.scaled[,c(-1,-2)]
response = target = whole_dataset.scaled$Happiness.Score

set.seed(7)
n = nrow(whole_dataset)
train_index=sample(n,n*0.9)

train_features=predictors[train_index,]
train_response=response[train_index]
print(paste("Shape of Training data is :", dim(train_features)[1], "rows and", dim(train_features)[2], "columns."))

test_features=predictors[-train_index,]
test_response=response[-train_index]

print(paste("Shape of Testing data is :", dim(test_features)[1], "rows and", dim(test_features)[2], "columns."))


lin_reg = lm(train_response~., data=train_features)

print("Training MSE: ")
mean((predict(lin_reg, train_features) - train_response)^2)

print("Testing MSE: ")
mean((predict(lin_reg,test_features) - test_response)^2)

summary(lin_reg)

# see if there is multicolinearity
vif_values = vif(lin_reg)
vif_values
barplot(vif_values, main = "VIF Values", horiz = TRUE)

# Sort VIF values in descending order for better visualization
sorted_vif <- sort(vif_values, decreasing = TRUE)

# Create a bar plot with improved visualization
barplot(sorted_vif, 
        horiz = TRUE, 
        main = "VIF Values for Multicollinearity Check",
        xlab = "VIF",
        col = "skyblue",  # Change bar color
        border = "black",  # Add black borders to bars
        xlim = c(0, max(sorted_vif) * 1.2),  # Extend x-axis limit for better view
        las = 1,  # Rotate y-axis labels horizontally
        names.arg = names(sorted_vif),  # Use variable names as labels
        cex.names = 0.7  # Reduce label size for better readability
)
```


# Linear Regression without Life Expectancy
```{r}
# scale the data
# scaled_whole_dataset = as.vector(scale(whole_dataset))
whole_dataset.scaled = as.data.frame(scale(whole_dataset))

# attach(scaled_whole_dataset)
predictors = whole_dataset.scaled[,c(-1,-4)]
response = target = whole_dataset.scaled$Happiness.Score

set.seed(7)
n = nrow(whole_dataset)
train_index=sample(n,n*0.9)

train_features=predictors[train_index,]
train_response=response[train_index]
print(paste("Shape of Training data is :", dim(train_features)[1], "rows and", dim(train_features)[2], "columns."))

test_features=predictors[-train_index,]
test_response=response[-train_index]

print(paste("Shape of Testing data is :", dim(test_features)[1], "rows and", dim(test_features)[2], "columns."))


lin_reg = lm(train_response~., data=train_features)


print("Training MSE: ")
mean((predict(lin_reg, train_features) - train_response)^2)

print("Testing MSE: ")
mean((predict(lin_reg,test_features) - test_response)^2)

summary(lin_reg)



# see if there is multicolinearity
vif_values = vif(lin_reg)
vif_values
barplot(vif_values, main = "VIF Values", horiz = TRUE)

# Sort VIF values in descending order for better visualization
sorted_vif <- sort(vif_values, decreasing = TRUE)

# Create a bar plot with improved visualization
barplot(sorted_vif, 
        horiz = TRUE, 
        main = "VIF Values for Multicollinearity Check",
        xlab = "VIF",
        col = "skyblue",  # Change bar color
        border = "black",  # Add black borders to bars
        xlim = c(0, max(sorted_vif) * 1.2),  # Extend x-axis limit for better view
        las = 1,  # Rotate y-axis labels horizontally
        names.arg = names(sorted_vif),  # Use variable names as labels
        cex.names = 0.7  # Reduce label size for better readability
)
```


# Lasso Regression Model

```{r}

cv_lasso = cv.glmnet(as.matrix(train_features), train_response, alpha = 1, nfolds=20)

 #lambda that minimizes cross validation error
lambda_best=cv_lasso$lambda.min

print(lambda_best)


plot(cv_lasso)

lasso_reg = glmnet(as.matrix(train_features), train_response, alpha = 1, lambda = lambda_best)  # alpha = 1 for Lasso

print("Training MSE: ")
mean((predict(lasso_reg, as.matrix(train_features)) - train_response)^2)

print("Testing MSE: ")
mean((predict(lasso_reg, as.matrix(test_features)) - test_response)^2)

coef(lasso_reg)
# summary(lass_reg)


plot(lasso_reg, xvar = "lambda")




# #use fitted best model to make predictions
# y_predicted <- predict(lasso_reg, s = lambda_best, newx = as.matrix(train_features))
# 
# #find SST and SSE
# sst <- sum((train_response - mean(train_response))^2)
# sse <- sum((y_predicted - train_response)^2)
# 
# #find R-Squared
# rsq <- 1 - sse/sst
# rsq
# 
# 
# lasso_adj_r_squared <- 1 - ((1 - rsq) * (nrow(train_response) - 1) / (nrow(train_response) - ncol(train_response) - 1))
# lasso_adj_r_squared



```

```{r}
# see if there is multicolinearity
# vif_values = vif(lasso_reg, x =as.matrix(train_features))
vif_lasso <- vif(lasso_reg, x = as.matrix(train_features))


barplot(vif_values, main = "VIF Values", horiz = TRUE)

# Sort VIF values in descending order for better visualization
sorted_vif <- sort(vif_values, decreasing = TRUE)

# Create a bar plot with improved visualization
barplot(sorted_vif, 
        horiz = TRUE, 
        main = "VIF Values for Multicollinearity Check",
        xlab = "VIF",
        col = "skyblue",  # Change bar color
        border = "black",  # Add black borders to bars
        xlim = c(0, max(sorted_vif) * 1.2),  # Extend x-axis limit for better view
        las = 1,  # Rotate y-axis labels horizontally
        names.arg = names(sorted_vif),  # Use variable names as labels
        cex.names = 0.7  # Reduce label size for better readability
)


```



# Ridge Regression Model

```{r}

lambda_values <- seq(0.1, 10000, length = 100000)

#we want to obtain optimal (best) lambda using cross-validation, we use cv.glmnet
# cv_ridge = cv.glmnet(as.matrix(train_features), train_response, alpha = 0, lambda = lambda_values,nfolds = 20)

cv_ridge = cv.glmnet(as.matrix(train_features), train_response, alpha = 0, nfolds = 20)

 #lambda that minimizes cross validation error
lambda_best=cv_ridge$lambda.min

print(lambda_best)

plot(cv_ridge)

ridge_reg = glmnet(as.matrix(train_features), train_response, lambda = lambda_best,alpha = 0)  # alpha = 0 for ridge

print("Training MSE: ")
mean((predict(ridge_reg, as.matrix(train_features)) - train_response)^2)

print("Testing MSE: ")
mean((predict(ridge_reg, as.matrix(test_features)) - test_response)^2)

coef(ridge_reg)
# summary(ridge_reg)


plot(ridge_reg, xvar = "lambda")
```

# K-Nearest Neighbour Model

```{r}
knn_model = train(x = train_features, y=train_response, method = "kknn", preProcess = c("center", "scale"))
# summary(knn_model)

summary(knn_model)
```

# Comparison of Results
```{r}

print("Training MSE")
mean((predict(lin_reg, train_features) - train_response)^2)
mean((predict(lasso_reg, as.matrix(train_features)) - train_response)^2)
mean((predict(ridge_reg, as.matrix(train_features)) - train_response)^2)
mean((predict(knn_model, train_features) - train_response)^2)

print("Test MSE")
mean((predict(lin_reg, test_features) - test_response)^2)
mean((predict(lasso_reg, as.matrix(test_features)) - test_response)^2)
mean((predict(ridge_reg, as.matrix(test_features)) - test_response)^2)
mean((predict(knn_model, test_features) - test_response)^2)

# summary(knn_model)$adj.r.squared
```

| Models               | Train MSE | Test MSE | Adj. R^2
|----------------------|-----------|----------|----------|
| Linear Regression    |  0.45     |  0.51    | 62.61
| Lasso Regression     |  0.45     |  0.52    |
| Ridge Regression     |  0.95     |  1.11    |
| KNN                  |  0.20     |  0.17    |

## Plots
```{r}

```
